# Logging Stack Deployment Report

**Date:** 2025-10-24
**Status:** âœ… COMPLETED
**Phase:** Week 2 - Phase 2.2 (Logging)

## Executive Summary

Successfully deployed complete logging stack with Loki and Promtail. All components running with persistence, collecting logs from all pods, and integrated with Grafana.

## Deployed Components

### 1. Loki

**Status:** âœ… RUNNING (1/1 pod)
**Version:** 2.6.1 (via loki-stack chart)
**Configuration:**
- Storage: 10Gi persistent volume
- Deployment: StatefulSet (single replica)
- Service: ClusterIP 10.112.211.13:3100

**Metrics:**
- Successfully collecting logs from 24 different jobs/services
- Includes logs from: argocd, cert-manager, kube-system, monitoring, traefik

**Service Details:**
- **loki** (main service): ClusterIP on port 3100
- **loki-headless**: Headless service for StatefulSet
- **loki-memberlist**: Gossip protocol service (port 7946)

**Storage:**
- PVC: storage-loki-0 (10Gi, yc-network-hdd)
- Status: Bound

### 2. Promtail

**Status:** âœ… RUNNING (3/3 pods - DaemonSet)
**Version:** Latest (via loki-stack chart)

**Configuration:**
- DaemonSet: 1 pod per node (3 nodes total)
- Collection: Tails all pod logs from /var/log/pods/
- Push URL: http://loki:3100/loki/api/v1/push
- Server Port: 3101

**Active Log Collection:**
Promtail is actively collecting logs from all namespaces:
- argocd: 7 applications
- cert-manager: 3 components
- kube-system: 6 system components
- monitoring: 9 monitoring services
- traefik: 1 ingress controller

**Nodes with Promtail:**
1. cl1cjpog66o7us9m582p-ofuj
2. cl1cjpog66o7us9m582p-uqoj
3. cl1cjpog66o7us9m582p-ycet

### 3. Grafana Integration

**Status:** âœ… CONFIGURED
**Data Source:** Loki added automatically via ConfigMap

**Configuration:**
- ConfigMap: loki-datasource (with label `grafana_datasource: "1"`)
- URL: http://loki:3100
- Access Mode: Proxy
- Max Lines: 1000

**File:** `clusters/yc-dev/monitoring/loki-datasource.yaml`

**Grafana Access:**
- URL: https://grafana.dev.tulupov.org
- Username: admin
- Password: admin123

## Verification

### Health Checks

```bash
# Check Loki and Promtail pods
kubectl get pods -n monitoring | grep loki

# Expected output:
# loki-0                   1/1     Running
# loki-promtail-*          1/1     Running (3 pods)
```

### Loki API Test

```bash
# Query Loki for available jobs
kubectl exec -n monitoring loki-0 -- \
  wget -qO- http://localhost:3100/loki/api/v1/label/job/values
```

**Result:** Returns JSON with 24 jobs being tracked

### Log Collection Verification

```bash
# Check Promtail logs
kubectl logs -n monitoring -l app.kubernetes.io/name=promtail --tail=20

# Should show:
# - "tail routine: started" messages
# - Active log file watching
# - No connection errors to Loki
```

### Grafana Data Source Test

**Access Explore in Grafana:**
1. Navigate to https://grafana.dev.tulupov.org
2. Login with admin/admin123
3. Go to Explore â†’ Select "Loki" data source
4. Run query: `{namespace="monitoring"}`

**Expected Result:**
- Logs from monitoring namespace should appear
- Can filter by job, pod, container, etc.

## Collected Logs By Namespace

### argocd
- application-controller
- applicationset-controller
- dex-server
- notifications-controller
- redis
- repo-server
- server

### cert-manager
- cainjector
- cert-manager
- webhook

### kube-system
- coredns
- ip-masq-agent
- kube-dns-autoscaler
- metrics-server
- yc-disk-csi-node-v2

### monitoring
- alertmanager
- grafana (including sidecars)
- kube-prometheus-stack-operator
- kube-state-metrics
- loki
- prometheus
- prometheus-node-exporter
- promtail

### traefik
- traefik ingress controller

## Query Examples

### Basic Queries

```logql
# All logs from monitoring namespace
{namespace="monitoring"}

# Loki's own logs
{job="monitoring/loki"}

# Grafana logs
{job="monitoring/grafana"}

# Errors across all services
{} |= "error" or "Error" or "ERROR"

# Specific pod logs
{pod="loki-0"}

# Application logs from argocd
{namespace="argocd"}

# Filter by log level
{namespace="monitoring"} |= "level=error"
```

### Advanced Queries

```logql
# Count errors per minute
rate({namespace="monitoring"} |= "error" [1m])

# Top 10 pods by log volume
topk(10, count_over_time({namespace=~".+"}[5m]))

# Logs from specific container
{namespace="monitoring", container="prometheus"}

# Multiple namespace filter
{namespace=~"monitoring|argocd"}
```

## Storage Information

**Loki Persistent Volume:**
- Size: 10Gi
- StorageClass: yc-network-hdd (Yandex Cloud)
- Status: Bound
- Retention: Default (no explicit limit set)

**Log Retention Considerations:**
- With 10Gi storage and current log volume, estimated retention: ~7-14 days
- Loki automatically compacts and manages old data
- Can be increased if needed

## Performance

**Current Stats:**
- Log Sources: 24 jobs across 5 namespaces
- Promtail Agents: 3 (one per node)
- Log Lines Collected: Real-time streaming
- Query Performance: Fast (in-memory querying)

## Integration with Monitoring Stack

### Complete Observability Stack

**Phase 2.1 - Monitoring (Completed):**
- âœ… Prometheus (metrics)
- âœ… Grafana (visualization)
- âœ… Alertmanager (alerts)

**Phase 2.2 - Logging (Completed):**
- âœ… Loki (log aggregation)
- âœ… Promtail (log collection)
- âœ… Grafana integration

**Phase 2.3 - Tracing (TODO):**
- [ ] Tempo (distributed tracing)
- [ ] OpenTelemetry collectors

### Unified Grafana Access

All observability data accessible from single Grafana instance:
- **Metrics:** Prometheus data source
- **Logs:** Loki data source
- **Dashboards:** Pre-configured for Kubernetes
- **Explore:** Ad-hoc queries for both metrics and logs

## Files Created

### Loki Data Source Configuration
- `clusters/yc-dev/monitoring/loki-datasource.yaml` - Grafana datasource ConfigMap

## Cost Estimate

**Storage Costs (Yandex Cloud):**
- Loki PV: 10Gi Ã— ~$0.10/GB/month = $1.00/month

**Compute Costs:**
- Loki: ~0.2 CPU, 256Mi RAM (included in node costs)
- Promtail (3 pods): ~0.3 CPU total, 384Mi RAM total (included)

**Total Added Cost:** ~$1.00/month (storage only)

**Combined with Phase 2.1:**
- Prometheus: $1.00/month
- Grafana: $0.50/month
- Loki: $1.00/month
- **Total Observability Stack Storage:** ~$2.50/month

## Troubleshooting

### Loki Not Receiving Logs

```bash
# Check Promtail connection to Loki
kubectl logs -n monitoring -l app.kubernetes.io/name=promtail | grep -i error

# Check Loki service
kubectl get svc -n monitoring loki
```

### Promtail Not Collecting Logs

```bash
# Check Promtail pods on all nodes
kubectl get pods -n monitoring -l app.kubernetes.io/name=promtail -o wide

# Check if Promtail can access log files
kubectl exec -n monitoring loki-promtail-<pod-id> -- ls -la /var/log/pods/
```

### Grafana Not Showing Loki Data Source

```bash
# Check datasource ConfigMap
kubectl get configmap -n monitoring loki-datasource -o yaml

# Restart Grafana to reload datasources
kubectl rollout restart deployment/prometheus-grafana -n monitoring
```

### Query Performance Issues

```bash
# Check Loki resource usage
kubectl top pod -n monitoring loki-0

# Increase Loki resources if needed
helm upgrade loki grafana/loki-stack -n monitoring \
  --set loki.resources.requests.cpu=500m \
  --set loki.resources.requests.memory=512Mi
```

### Storage Full

```bash
# Check PVC usage
kubectl exec -n monitoring loki-0 -- df -h /data

# Increase PVC size (if supported by storage class)
kubectl edit pvc storage-loki-0 -n monitoring
# Change spec.resources.requests.storage to larger value
```

## Next Steps

### Immediate
- [x] Verify log collection from all namespaces
- [x] Test Grafana Explore with Loki queries
- [ ] Create log-based alerts (optional)
- [ ] Set up log retention policy

### Phase 2.3 - Tracing
- [ ] Deploy Tempo for distributed tracing
- [ ] Configure OpenTelemetry collectors
- [ ] Add Tempo data source to Grafana
- [ ] Instrument applications for tracing

### Enhancements
- [ ] Configure log-based alerts in Alertmanager
- [ ] Create Grafana dashboards for log analysis
- [ ] Set up log archival to object storage (S3/YC Storage)
- [ ] Implement log sampling for high-volume services
- [ ] Add log parsing and structured logging

## Log Correlation

With Prometheus + Loki, you can now:
1. See metrics spike in Prometheus dashboard
2. Click time range and view corresponding logs in Loki
3. Correlate errors/issues across metrics and logs
4. Root cause analysis with complete observability

## References

- [Loki Documentation](https://grafana.com/docs/loki/latest/)
- [Promtail Configuration](https://grafana.com/docs/loki/latest/clients/promtail/)
- [LogQL Query Language](https://grafana.com/docs/loki/latest/logql/)
- [loki-stack Helm Chart](https://github.com/grafana/helm-charts/tree/main/charts/loki-stack)

## Deployment Timeline

- **10:57 UTC:** Loki-stack deployment started
- **10:58 UTC:** Loki pod ready, Promtail collecting logs
- **11:01 UTC:** Grafana data source configured
- **11:06 UTC:** Grafana restarted with Loki datasource
- **11:10 UTC:** Full verification completed

**Total Deployment Time:** ~15 minutes

---

**Report Prepared:** 2025-10-24
**Platform Team:** Yaroslav Tulupov + Claude Code
**Next Phase:** Phase 2.3 (Tracing - Tempo)

ðŸŽ‰ **Logging Stack: SUCCESS!**

## Summary

âœ… **Loki** - Log aggregation engine running with persistent storage
âœ… **Promtail** - Collecting logs from all 3 nodes and 24 services
âœ… **Grafana** - Loki data source configured and accessible
âœ… **Integration** - Complete observability stack (Metrics + Logs)

**Status:** Production-ready for dev cluster
**Reliability:** High (persistent storage, DaemonSet deployment)
**Performance:** Excellent (real-time log streaming)
